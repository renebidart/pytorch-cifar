{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch-cifar models have 3x3 instead of 7x7 for the first layer. This is significantly better for smaller images??? like cifar. Without this the accuracy is <=90%\n",
    "\n",
    "Comparing the standard kuangliu models to my rwightman version to training this using lightning (weight-sharing repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 3, 3])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([16, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([16])\n",
      "layer2.0.bn1.bias torch.Size([16])\n",
      "layer2.0.conv2.weight torch.Size([16, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([16])\n",
      "layer2.0.bn2.bias torch.Size([16])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([16, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([16])\n",
      "layer2.1.bn1.bias torch.Size([16])\n",
      "layer2.1.conv2.weight torch.Size([16, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([16])\n",
      "layer2.1.bn2.bias torch.Size([16])\n",
      "layer3.0.conv1.weight torch.Size([32, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([32])\n",
      "layer3.0.bn1.bias torch.Size([32])\n",
      "layer3.0.conv2.weight torch.Size([32, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([32])\n",
      "layer3.0.bn2.bias torch.Size([32])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([32, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([32])\n",
      "layer3.1.bn1.bias torch.Size([32])\n",
      "layer3.1.conv2.weight torch.Size([32, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([32])\n",
      "layer3.1.bn2.bias torch.Size([32])\n",
      "layer4.0.conv1.weight torch.Size([64, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([64])\n",
      "layer4.0.bn1.bias torch.Size([64])\n",
      "layer4.0.conv2.weight torch.Size([64, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([64])\n",
      "layer4.0.bn2.bias torch.Size([64])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([64, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([64])\n",
      "layer4.1.bn1.bias torch.Size([64])\n",
      "layer4.1.conv2.weight torch.Size([64, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([64])\n",
      "layer4.1.bn2.bias torch.Size([64])\n",
      "fc.weight torch.Size([10, 512])\n",
      "fc.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "ws_amt = 8\n",
    "model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=ws_amt, channel_factor=1,\n",
    "                         num_classes=10, cifar=True)\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    print(name, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 3, 3])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([16, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([16])\n",
      "layer2.0.bn1.bias torch.Size([16])\n",
      "layer2.0.conv2.weight torch.Size([16, 16, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([16])\n",
      "layer2.0.bn2.bias torch.Size([16])\n",
      "layer2.0.downsample.0.weight torch.Size([16, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([16])\n",
      "layer2.0.downsample.1.bias torch.Size([16])\n",
      "layer2.1.conv1.weight torch.Size([16, 16, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([16])\n",
      "layer2.1.bn1.bias torch.Size([16])\n",
      "layer2.1.conv2.weight torch.Size([16, 16, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([16])\n",
      "layer2.1.bn2.bias torch.Size([16])\n",
      "layer3.0.conv1.weight torch.Size([32, 16, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([32])\n",
      "layer3.0.bn1.bias torch.Size([32])\n",
      "layer3.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([32])\n",
      "layer3.0.bn2.bias torch.Size([32])\n",
      "layer3.0.downsample.0.weight torch.Size([32, 16, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([32])\n",
      "layer3.0.downsample.1.bias torch.Size([32])\n",
      "layer3.1.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([32])\n",
      "layer3.1.bn1.bias torch.Size([32])\n",
      "layer3.1.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([32])\n",
      "layer3.1.bn2.bias torch.Size([32])\n",
      "layer4.0.conv1.weight torch.Size([64, 32, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([64])\n",
      "layer4.0.bn1.bias torch.Size([64])\n",
      "layer4.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([64])\n",
      "layer4.0.bn2.bias torch.Size([64])\n",
      "layer4.0.downsample.0.weight torch.Size([64, 32, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([64])\n",
      "layer4.0.downsample.1.bias torch.Size([64])\n",
      "layer4.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([64])\n",
      "layer4.1.bn1.bias torch.Size([64])\n",
      "layer4.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([64])\n",
      "layer4.1.bn2.bias torch.Size([64])\n",
      "fc.weight torch.Size([10, 64])\n",
      "fc.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=1, channel_factor=ws_amt,\n",
    "                         num_classes=10, cifar=True)\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    print(name, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_amt = 8\n",
    "ws_model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=ws_amt, channel_factor=1,\n",
    "                         num_classes=10, cifar=True)\n",
    "\n",
    "save_path = './saved_models_old/cifar10/resnet18bn_ws'+str(ws_amt)+'_ch1'\n",
    "checkpoint = torch.load(save_path + '/ckpt.pth')\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint['net'].items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "ws_model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without redundancy (fraction instead of ws)\n",
    "small_model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=1, channel_factor=ws_amt,\n",
    "                         num_classes=10, cifar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws model: layer2.0.conv2.weight torch.Size([16, 128, 3, 3])\n",
      "frac model                    : torch.Size([16, 16, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 16, 16, 3, 3])\n",
      "weight.size():                  torch.Size([16, 16, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer2.1.conv1.weight torch.Size([16, 128, 3, 3])\n",
      "frac model                    : torch.Size([16, 16, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 16, 16, 3, 3])\n",
      "weight.size():                  torch.Size([16, 16, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer2.1.conv2.weight torch.Size([16, 128, 3, 3])\n",
      "frac model                    : torch.Size([16, 16, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 16, 16, 3, 3])\n",
      "weight.size():                  torch.Size([16, 16, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer3.0.conv1.weight torch.Size([32, 128, 3, 3])\n",
      "frac model                    : torch.Size([32, 16, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 32, 16, 3, 3])\n",
      "weight.size():                  torch.Size([32, 16, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer3.0.conv2.weight torch.Size([32, 256, 3, 3])\n",
      "frac model                    : torch.Size([32, 32, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 32, 32, 3, 3])\n",
      "weight.size():                  torch.Size([32, 32, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer3.1.conv1.weight torch.Size([32, 256, 3, 3])\n",
      "frac model                    : torch.Size([32, 32, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 32, 32, 3, 3])\n",
      "weight.size():                  torch.Size([32, 32, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer3.1.conv2.weight torch.Size([32, 256, 3, 3])\n",
      "frac model                    : torch.Size([32, 32, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 32, 32, 3, 3])\n",
      "weight.size():                  torch.Size([32, 32, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer4.0.conv1.weight torch.Size([64, 256, 3, 3])\n",
      "frac model                    : torch.Size([64, 32, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 64, 32, 3, 3])\n",
      "weight.size():                  torch.Size([64, 32, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer4.0.conv2.weight torch.Size([64, 512, 3, 3])\n",
      "frac model                    : torch.Size([64, 64, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 64, 64, 3, 3])\n",
      "weight.size():                  torch.Size([64, 64, 3, 3])\n",
      "-------------------------------\n",
      "ws model: layer4.1.conv1.weight torch.Size([64, 512, 3, 3])\n",
      "frac model                    : torch.Size([64, 64, 3, 3])\n",
      "wgt_corresp_to_repeat.size():   torch.Size([8, 64, 64, 3, 3])\n",
      "weight.size():                  torch.Size([64, 64, 3, 3])\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "small_state_dict = new_state_dict\n",
    "\n",
    "# sum repeated weights. format [n_out, n_in, k0, k1] -> [n_out, n_in/ws_amt, k0, k1] \n",
    "for name, p in ws_model.named_parameters():\n",
    "    # Skipped the first conv, first block, and the final FC layer\n",
    "    if (name.split('.')[0] not in ['conv1', 'bn1', 'layer1', 'fc'] and \n",
    "          ('layer2.0.conv1' not in name) and ('layer4.1.conv2' not in name)):\n",
    "            \n",
    "        # only repeat conv weights (bias is on the output, which is already small)\n",
    "        if 'conv' in name:\n",
    "            print('ws model:', name, p.size())\n",
    "            print('frac model                    :', small_model.state_dict()[name].size())\n",
    "            wgt_corresp_to_repeat = []\n",
    "            \n",
    "            for i in range(ws_amt):\n",
    "                idxs = torch.LongTensor(list(range(i, p.size()[1], ws_amt)))\n",
    "#                 print(idxs)\n",
    "                stuff = torch.index_select(p, dim=1, index=idxs)\n",
    "                wgt_corresp_to_repeat.append(stuff)\n",
    "#                 print('stuff.size()', stuff.size())\n",
    "            wgt_corresp_to_repeat = torch.stack(wgt_corresp_to_repeat, 0)\n",
    "            print('wgt_corresp_to_repeat.size():  ', wgt_corresp_to_repeat.size())\n",
    "            weight = wgt_corresp_to_repeat.sum(dim=0)\n",
    "            print('weight.size():                 ', weight.size())\n",
    "            print('-------------------------------')\n",
    "            small_state_dict[name] = weight\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNetLight:\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 16, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([10, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-0ae053fa6f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/LSSL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNetLight:\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 16, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 32, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([10, 64])."
     ]
    }
   ],
   "source": [
    "small_model.load_state_dict(small_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNetLight:\n\tsize mismatch for layer2.0.conv2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.1.conv2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer3.0.conv2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.1.conv2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer4.0.conv2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.1.conv2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([10, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-59adc2b9a2d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove `module.`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msmall_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/LSSL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNetLight:\n\tsize mismatch for layer2.0.conv2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer2.1.conv2.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer3.0.conv2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer3.1.conv2.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer4.0.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer4.0.conv2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer4.1.conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.1.conv2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([10, 256])."
     ]
    }
   ],
   "source": [
    "small_model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=1, channel_factor=ws_amt,\n",
    "                         num_classes=10, cifar=True)\n",
    "\n",
    "# save_path = './saved_models_old/cifar10/resnet18bn_ws'+str(ws_amt)+'_ch1'\n",
    "# checkpoint = torch.load(save_path + '/ckpt.pth')\n",
    "\n",
    "# state_dict = {}\n",
    "# for k, v in checkpoint['net'].items():\n",
    "#     name = k[7:] # remove `module.`\n",
    "#     state_dict[name] = v\n",
    "# small_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-eaad509002ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# sum repeated weights. format [n_out, n_in, k0, k1] -> [n_out, n_in/ws_amt, k0, k1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_dict' is not defined"
     ]
    }
   ],
   "source": [
    "small_state_dict = state_dict\n",
    "\n",
    "# sum repeated weights. format [n_out, n_in, k0, k1] -> [n_out, n_in/ws_amt, k0, k1] \n",
    "for name, p in model.named_parameters():\n",
    "    # Skipped the first conv, first block, and the final FC layer\n",
    "    if name.split('.')[0] not in ['conv1', 'bn1', 'layer1', 'fc']:\n",
    "        print(name, p.size())\n",
    "        # only repeat conv weights (bias is on the output, which is already small)\n",
    "        if 'conv' in name:\n",
    "            print(p.size())            \n",
    "            wgt_corresp_to_repeat = []\n",
    "            \n",
    "            for i in range(ws_amt):\n",
    "                idxs = torch.LongTensor(list(range(i, p.size()[1], ws_amt)))\n",
    "                print(idxs)\n",
    "                stuff = torch.index_select(p, dim=1, index=idxs)\n",
    "                wgt_corresp_to_repeat.append(stuff)\n",
    "                print('stuff.size()', stuff.size())\n",
    "            wgt_corresp_to_repeat = torch.stack(wgt_corresp_to_repeat, 0)\n",
    "            print('wgt_corresp_to_repeat.size()', wgt_corresp_to_repeat.size())\n",
    "            weight = wgt_corresp_to_repeat.sum(dim=0)\n",
    "            print('weight.size()', weight.size())\n",
    "            small_state_dict[name] = weight\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 3, 3])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([64])\n",
      "layer2.0.bn1.bias torch.Size([64])\n",
      "layer2.0.conv2.weight torch.Size([64, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([64])\n",
      "layer2.0.bn2.bias torch.Size([64])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([64, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([64])\n",
      "layer2.1.bn1.bias torch.Size([64])\n",
      "layer2.1.conv2.weight torch.Size([64, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([64])\n",
      "layer2.1.bn2.bias torch.Size([64])\n",
      "layer3.0.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([128])\n",
      "layer3.0.bn1.bias torch.Size([128])\n",
      "layer3.0.conv2.weight torch.Size([128, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([128])\n",
      "layer3.0.bn2.bias torch.Size([128])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([128, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([128])\n",
      "layer3.1.bn1.bias torch.Size([128])\n",
      "layer3.1.conv2.weight torch.Size([128, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([128])\n",
      "layer3.1.bn2.bias torch.Size([128])\n",
      "layer4.0.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([256])\n",
      "layer4.0.bn1.bias torch.Size([256])\n",
      "layer4.0.conv2.weight torch.Size([256, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([256])\n",
      "layer4.0.bn2.bias torch.Size([256])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([256, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([256])\n",
      "layer4.1.bn1.bias torch.Size([256])\n",
      "layer4.1.conv2.weight torch.Size([256, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([256])\n",
      "layer4.1.bn2.bias torch.Size([256])\n",
      "fc.weight torch.Size([10, 512])\n",
      "fc.bias torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    i\n",
    "    print(name, p.size())\n",
    "\n",
    "\n",
    "\n",
    "save_path = './saved_models_old/cifar10/resnet18bn_ws'+str(ws_amt)+'_ch1'\n",
    "checkpoint = torch.load(save_path + '/ckpt.pth')\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint['net'].items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight sharing beats removing filters directly\n",
    "* Even training both for 350 epochs, plenty of time for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 acc frac: 95.49 acc ws: 95.49\n",
      "2 acc frac: 94.79 acc ws: 94.98\n",
      "4 acc frac: 93.9 acc ws: 94.62\n",
      "8 acc frac: 92.27 acc ws: 94.02\n",
      "16 acc frac: 89.51 acc ws: 93.31\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 4, 8, 16]:\n",
    "    save_path = './saved_models/cifar10/resnet18_ws1_ch'+str(i)\n",
    "    save_path_ws = './saved_models/cifar10/resnet18_ws'+str(i)+'_ch1'\n",
    "    checkpoint = torch.load(save_path + '/ckpt.pth')\n",
    "    acc = checkpoint['acc']\n",
    "    checkpoint = torch.load(save_path_ws + '/ckpt.pth')\n",
    "    acc_ws = checkpoint['acc']\n",
    "    print(f'{i} acc frac: {acc} acc ws: {acc_ws}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=1, channel_factor=1,\n",
    "                         num_classes=10, cifar=False)\n",
    "cifar_model = ResNet18()\n",
    "\n",
    "\n",
    "imagenet_model_names = []\n",
    "imagenet_model_sizes = []\n",
    "for name, param in imagenet_model.named_parameters():\n",
    "    imagenet_model_names.append(name)\n",
    "    imagenet_model_sizes.append(param.size())\n",
    "    \n",
    "cifar_model_names = []\n",
    "cifar_model_sizes = []\n",
    "for name, param in cifar_model.named_parameters():\n",
    "    cifar_model_names.append(name)\n",
    "    cifar_model_sizes.append(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 64, 32, 32])\n",
      "1 torch.Size([2, 64, 32, 32])\n",
      "2 torch.Size([2, 128, 16, 16])\n",
      "3 torch.Size([2, 256, 8, 8])\n",
      "4 torch.Size([2, 512, 4, 4])\n",
      "pool torch.Size([2, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "imagenet_model = ResNetLight(BasicBlockLight, [2, 2, 2, 2], ws_factor=1, channel_factor=1,\n",
    "                         num_classes=10, cifar=True)\n",
    "\n",
    "imagenet_model.to('cuda')\n",
    "y = imagenet_model(torch.randn(2,3,32,32).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSSL",
   "language": "python",
   "name": "lssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
